# 模型评估与选择
## 1. 经验误差与过拟合
- **错误率** 分类错误的样本数$m$占样本总数$a$的比例, $E=a/m$
- **精度**，$=1-a/m$
- **误差**，学习器的实际预测输出与样本的真实输出之间的差异；学习器在训练集上的误差称为**训练误差**或**经验误差**，在新样本上的误差称为**泛化误差**
- **过拟合 overfitting** 学习器把训练样本学得“太好了”，很可能已经把训练样本自身的一些特点当做了所有潜在样本都会有的一般性质，这样会导致*泛化能力下降*。与之相对的是**欠拟合 underfitting**。
---
## 2. 评估方法
通常的，我们只有一个包含$m$个样例的数据集$D=\{(x_1,y_1),(x_2,y_2),...(x_m,y_m)\}$，既要训练又要测试。所以，通过对$D$进行适当的处理，==从中产生出训练集$S$和测试集$T$==。
### 2.1 留出法
hold out 直接将数据集$D$划分为两个互斥的集合，其中一个作为训练集，另一个作为测试集
>注意：
>1. 训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响。**分层采样**——保留类别比例的采样方式
>1. 单次使用留出法得到的估计结果往往不够稳定可靠，一般采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。
>1. 常见的，将大约2/3~4/5的样本用于训练，剩余样本用于测试。

### 2.2 交叉验证法
cross validation
步骤：
1. 先将数据集$D$划分外$k$个大小相似的互斥子集$D_i$。每个子集都尽可能保持数据分布的一致性，采用分层采样。
2. 然后每次用$k-1$个子集的并集作为训练集，余下的那个子集作为测试集；
3. 这样就可获得$k$组训练/测试集，从而进行$k$次训练和测试，最终返回的是这个$k$个测试结果的均值
> $k$通常取值是10，此时称为10折交叉验证，其他可取5,20等；随机重复$p$次，最终评估结果是这$p$次$k$折交叉验证结果的均值。

- 特殊的，假定数据集$D$中包含$m$个样本，若令$k=m$，则称为**留一法 LOD**。留一法的结果往往被认为比较准确；但在数据集比较大时，计算开销太大不适用。
### 2.3 自助法
bootstrapping
前面两种所使用的训练集比$D$小，这必然会引入一些因训练样本规模不同而导致的估计误差；而留一法计算复杂度又太高，所以在减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计。
以自主采样法为基础，给定包含$m$个样本的数据集$D$，我们对他进行采样产生数据集$D'$:
1. 每次随机从$D$中挑选出一个样本，将其拷贝放入$D'$
2. 然后再将该样本放回初始数据集$D$中，是的该样本在下次采样时仍有可能被采到；
3. 这个过程重复执行$m$次后，我们就得到了包含$m$个样本的数据集$D'$
>初始数据集$D$中约有36.8%的样本未出现在采集数据集$D'$中

我们可将$D'$作为训练集，$D$ \ $D'$用作测试集。
自助法在数据集**较小、难以有效划分训练/测试集**时很有用；在数据量充足的情况下，留出法和交叉验证法更常用。
### 2.4 调参和最终模型

调参 parameter tuning

对每个参数选定一个**范围**和**变化步长**，最终在这变化的几个数中产生选定值。
给定包含$m$个样本的数据集$D$，在模型评估与选择过程中由于需要留出一部分数据进行评估测试，实时上我们只使用了一部分数据训练模型。因此，在模型选择完成后，学习算法和参数配置已选定，此时应该用数据集$D$重新训练模型。是的在训练过程中使用了所有m个样本。
- 为了区分，我们通常把学得模型在实际使用中遇到的数据称为**测试数据**，模型评估与选择中用于评估测试的数据集常称为**验证集**

## 3. 性能度量
对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还有衡量模型泛化能力的评价标准，这就是**性能度量 performance measure**。
>回归任务最常用的性能度量是**均方误差**

### 3.1 错误率与精度

### 3.2 查准率、查全率与$F1$

### 3.3 ROC 与 AUC

### 3.4 代价敏感错误率与代价曲线
